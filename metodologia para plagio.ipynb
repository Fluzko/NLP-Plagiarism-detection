{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DETECTOR DE PLAGIO\n",
    "===============\n",
    "DETECCION MEDIANTE SENTENCES EMBEDDINGS Y SUS CORRESPONDIENTES BIGRAMAS\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contexto\n",
    "Este trabajo practico fue realizado para la materia Procesamiento del Lenguaje Natural, de la carrera Ingenieria en Sistemas de Informacion, UTN FRBA  \n",
    "  \n",
    "### Descripcion\n",
    "A grandes rasgos podemos distinguir dos grandes modulos:  \n",
    " - **Preprocesamiento:** En esta seccion el sistema se encargara de tomar todos los archivos (dataset) que se encuentren en un directorio especificado, extrayendo su texto por medio del parser adecuado para su extension, soportando actualmente .doc | .docx | .pdf.\n",
    " - **Deteccion:** Por medio del servicio de deteccion se puede cargar un archivo, el cual sera analizado y contrastado con el set de datos preprocesados con el que se cuenta, pudiendo obtener: procentaje de plagio, oraciones que fueron plagiadas y nombre del archivo del dataset\n",
    "El proyecto expone dos endpoints:\n",
    " - **[POST] /preprocess**, el cual nos permite preprocesar todos los archivos del dataset y tener listo el sistema\n",
    " - **[POST] /detect**, el cual nos permite pasarle un archivo por body de la manera -> *file: \"nombre_archivo.extension\"*. Este mismo debera ser pasado como form-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metodologia  \n",
    "La metodologia utilizada para realizar la deteccion, es la que se propone en el paper *[An Improved SRL based Plagiarism Detection Technique using\n",
    "Sentence Ranking](https://www.sciencedirect.com/science/article/pii/S1877050915000794)*, en el que los autores especifican tres pasos o etapas bien definidas\n",
    " - Pre-processing *(Pre-procesamiento)*\n",
    " - Candidate Retrieval *(Eleccion de candidatos)*\n",
    " - Sentence Ranking *(Ranking de oraciones)*\n",
    " - Semantic Role Labeling *(Etiquetado semantico de las oraciones)*\n",
    " - Similarity Detection *(Deteccion de plagio o similitud)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Pre-processing (Pre-procesamiento)**\n",
    "Esta accion inicial nos permite deshacernos de palabras redundantes llamadas 'stopwords' que no aportan contenido al documento o a la idea general.  \n",
    "  \n",
    "Para realizar esto utilizaremos la libreria NLTK, con los agregados de las stopwords y punkt, ambas extenciones que nos serviran a la hora de detectar las stopwords y quitarlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/facundo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/facundo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "\n",
    "def preprocess_document(text):\n",
    "    return ' '.join([word.lower() for word in word_tokenize(text) if word.lower() not in spanish_stopwords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Candidate Retrieval (Eleccion de candidatos)**\n",
    "En esta seccion lo que se propone es obtener los textos, que podrian haber sido plagiados, de un corpus previamente analizado y preprocesado.  \n",
    "Este analisis se realizara mediante el metodo de Jaccard el cual nos dara un coeficiente de similitud en base a la siguiente ecuacion:\n",
    " - coeficiente = cantidad de intersecciones de bigramas / cantidad de la union de bigramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity_coefficient(suspect_n_grams, original_n_grams):\n",
    "    return len(suspect_n_grams.intersection(original_n_grams)) / len(suspect_n_grams.union(original_n_grams))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si bien esta funcion nos permite calcular el coeficiente, aun necesitamos poder calcular los bigramas de un texto.  \n",
    "Esto lo haremos mediante la combinacion de funcionalidades que nos ofrece la libreria de NLTK.  \n",
    "Por un lado utilizaremos *word_tokenize* la cual nos dara un array de palabras y por otro lado usaremos *bigrams* el cual nos generara un array de tuplas (bigramas) dado el array anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams, word_tokenize\n",
    "\n",
    "def get_text_bygrams(text):\n",
    "    return set(bigrams(word_tokenize(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ultimo, y ya teniendo esto, estamos en condiciones de poder decir si dos textos son plagio.  \n",
    "Como ultimo dato, necesitaremos un un coeficiente, el cual nos servira como punto de corte para decidir. A este coeficiente le llamaremos *jaccard_threshold*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_threshold = 0.2\n",
    "\n",
    "def may_be_plagiarism_of(suspect, original):\n",
    "    preprocessed_original = preprocess_document(original)\n",
    "    preprocessed_suspect = preprocess_document(suspect)\n",
    "    bigrams_original = get_text_bygrams(preprocessed_original)\n",
    "    bigrams_suspect = get_text_bygrams(preprocessed_suspect)\n",
    "    coefficient = jaccard_similarity_coefficient(bigrams_suspect, bigrams_original)\n",
    "    return coefficient > jaccard_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Sentence Ranking (Ranking de oraciones)**\n",
    "Este paso podemos dividirlo conceptualmente en dos bloques:\n",
    " - **Vectorizacion oraciones:** Este paso consiste en calcular los embeddings de una oracion. Esto graficamente seria como asignarle un vector a cada oracion analizada. Nos ayudaremos de un modelo pre entrenado que ofrece google y es parse de Tensorflow. Este se llama *universal-sentence-encoder-multilingual* en su verison 3. Una de las cosas interesantes que nos ofrece es la posibilidad de elegir la version multilenguaje, la cual se adapta al castellano entre otros. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero debemos asegurarnos de tener instalado tensorflow en una version mayor a la 2.0.0 y Tensorflow hub.  \n",
    "Esta informacion y mas, podemos encontrarla en el [apartado oficial de Tensorflow](https://tfhub.dev/google/universal-sentence-encoder-multilingual/3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_hub in /home/facundo/miniconda3/envs/plagiarism/lib/python3.7/site-packages (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/facundo/miniconda3/envs/plagiarism/lib/python3.7/site-packages (from tensorflow_hub) (1.18.5)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /home/facundo/miniconda3/envs/plagiarism/lib/python3.7/site-packages (from tensorflow_hub) (3.13.0)\n",
      "Requirement already satisfied: setuptools in /home/facundo/miniconda3/envs/plagiarism/lib/python3.7/site-packages (from protobuf>=3.8.0->tensorflow_hub) (50.3.0.post20201006)\n",
      "Requirement already satisfied: six>=1.9 in /home/facundo/miniconda3/envs/plagiarism/lib/python3.7/site-packages (from protobuf>=3.8.0->tensorflow_hub) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install tensorflow_text>=2.0.0rc0\n",
    "!pip install tensorflow_hub\n",
    "import tensorflow_text\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego podremos obtener el modelo del encoder para calcular los embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\")\n",
    "# Ejemplo:\n",
    "# embed('un texto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - **Comparacion mediante el metodo del coseno:** Utilizando la ecuacion que se muestra debajo, podemos calcular la similitud entre dos vectores. Expandiendo esto, podemos hacerlo con todo el array de embeddings dado por el modelo pre entrenado de Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SKLearn nos provee de esta funcion ya previamente desarrollada, por lo tanto solo basta con importarla y utilizarla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ultimo nos queda implementar una funcion que calcule los embeddings de ambos textos a comparar y que, con una adaptacion previa, se los pase al servicio de sklearn para que nos devuelva un coeficiente.  \n",
    "Nuevamente debemos proponer un threshold el cual nos servira para determinar con que nivel de exigencia decimos que una oracion es similar a otra. A este coeficiente lo llamaremos *cosine_similarity_threshold*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_threshold = 0.7\n",
    "\n",
    "def sentence_is_semantically_similar(suspect_sentence, original_sentence):\n",
    "    suspect_embedding = embed(suspect_sentence).numpy().reshape(1, 512)\n",
    "    original_embedding = embed(original_sentence).numpy().reshape(1, 512)\n",
    "    return cosine_similarity(suspect_embedding, original_embedding) >= cosine_similarity_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Semantic Role Labeling (Etiquetado semantico de las oraciones)**\n",
    "Consiste en asignar un proposito a las partes que componen a una oracion. Pudiendo determinar: \"qué\", \"quién\", \"donde\", \"cómo\", \"cuando\".\n",
    "Esta operacion se realizara sobre las obtenidas anteriormente, presentando similitudes relevantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Similarity Detection (Deteccion de plagio o similitud)**\n",
    "Por ultimo realizaremos la comparacion entre textos, esto implica separar por oraciones haciendo uso de la funcionalidad *sent_tokenize* que nos ofrece NLTK y comparando contra todo el resto del texto original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "def any_match(collection, function):\n",
    "    for element in collection:\n",
    "        if function(element):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def sentence_is_plagiarism(suspect_sentence, original_document):\n",
    "    return any_match(\n",
    "        sent_tokenize(original_document),\n",
    "        lambda original_sentence: sentence_is_semantically_similar(suspect_sentence, original_sentence)\n",
    "    )\n",
    "\n",
    "def plagiarised_sentences(suspect_document, original_document):\n",
    "    suspect_sentences = sent_tokenize(suspect_document)\n",
    "    return list(\n",
    "        filter(\n",
    "            lambda suspect_sentence: sentence_is_plagiarism(suspect_sentence, original_document),\n",
    "            suspect_sentences\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El calculo de procentaje de plagio se realiza con una simple relacion entre el total de oraciones y el la cantidad de oraciones que se determinaron como plagio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plagiarism_percentage(suspect_document, original_document):\n",
    "    n_plagiarised = len(plagiarised_sentences(suspect_document, original_document))\n",
    "    n_total = len(sent_tokenize(suspect_document))\n",
    "    return n_plagiarised / n_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente hacemos una prueba con un pequeno parrafo parafraseado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Puede ser plagio: True.\n",
      "  - Porcentaje de plagio: 0.5.\n",
      "  - Oraciones que realizan plagio: ['Me gusta la naranja.']\n"
     ]
    }
   ],
   "source": [
    "orig = \"Hola soy Pablo. Me gustaria una naranja.\"\n",
    "plag = \"Me gusta la naranja. Él se llama Pablo.\"\n",
    "puede_ser_plagio = may_be_plagiarism_of(plag, orig)\n",
    "porcentaje_de_plagio = plagiarism_percentage(plag, orig)\n",
    "oraciones_plagio = plagiarised_sentences(plag, orig)\n",
    "print(f\"  - Puede ser plagio: {puede_ser_plagio}.\")\n",
    "print(f\"  - Porcentaje de plagio: {porcentaje_de_plagio}.\")\n",
    "print(f\"  - Oraciones que realizan plagio: {oraciones_plagio}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
